{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial 2: MLP network & Convolutional network\n",
    "In this demonstration we will train a neural network to perform classification on the MNIST handwritten digits dataset. We will use the PyTorch Library, a high level API for building and training neural networks. \n",
    "\n",
    "We also import numpy, matplotlib, hiddenlayer and tqdm for visualization, so that we can take a better look at the inner workings of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import hiddenlayer as hl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here we define some hyperparameters that we will use for training. Later on we will change some parameters to see how they influence the training, but for now we will leave them as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "torch.manual_seed(1234) #Setting a seed for the random nuber generator in pyTorch\n",
    "learning_rate = 0.01\n",
    "batch_size = 48\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10 #How many times we loop through our training data\n",
    "MNIST_mean = 0.1307\n",
    "MNIST_std = 0.3081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST\n",
    "\n",
    "Next we will load the MNIST training and testing datasets. Using the mean and standard deviation of the MNIST dataset we normalize the data. The MNIST training set contains 60000 images and the test set contains 10000 images where each image is represented as 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((MNIST_mean,), (MNIST_std,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the data we loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(255 * np.transpose(npimg, (1, 2, 0)).astype(np.uint8))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False))\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Labels: ', ' '.join('%s, ' % (labels.data).cpu().numpy()[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "Now it's time to build our neural network. In the constructor we will define the pyTorch neural network components we wish to use, while in the forward function we define the data flow for the forward pass. For the backward pass pyTorch uses automatic differentiation to calculate the losses based on the graph structure we have defined in our forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    \"\"\"MLP network\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)           #Reshape input tensor.\n",
    "        x = F.relu(self.fc1(x))         #Fully connected hidden layer with 500 neurons.\n",
    "        x = self.fc2(x)                 #10 fully connected output neurons for our digits. \n",
    "        return x \n",
    "net = MLPNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the neural network\n",
    "\n",
    "Here is what the neural network graph we created looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    hl.transforms.Prune(\"Constant\"),\n",
    "    hl.transforms.Prune(\"Const\"),\n",
    "    hl.transforms.Fold(\"Linear > Relu\", \"Linear-Relu\"),\n",
    "    hl.transforms.Fold(\"Conv > Relu\", \"Conv-Relu\"),\n",
    "]\n",
    "hl_graph = hl.build_graph(net, torch.zeros([1, 1, 28, 28]), transforms=transforms)\n",
    "hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()\n",
    "hl_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing loops\n",
    "\n",
    "Now we will define our training and testing loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, history, canvas, oldHistory=None):\n",
    "    \"\"\"Training\"\"\"\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % (60000/(batch_size*10)) == 0 and batch != 0: # Print loss 10 times per epoch\n",
    "            history.log((epoch*len(train_loader.dataset) + batch * len(data)), loss=loss)\n",
    "            if oldHistory is None:\n",
    "                with canvas:\n",
    "                    canvas.draw_plot([history[\"loss\"]])\n",
    "            else:\n",
    "                with canvas:\n",
    "                    canvas.draw_plot([oldHistory[\"loss\"], history[\"loss\"]], labels=[\"MLP-loss\", \"CNN-loss\"])\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(epoch, batch * len(data), len(train_loader.dataset), loss.item()))\n",
    "\n",
    "def test(model, test_loader):\n",
    "    \"\"\"Testing\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Finally, we will run the training. We will use stochastic gradient descent (SGD) for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the network\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(net, train_loader, optimizer, epoch, history1, canvas1)\n",
    "    test(net, test_loader)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional Neural Network\n",
    "\n",
    "Next we will make some modifications to the network to see if we can improve our accuracy and/or speed up the training. We add two convolutional layers with max pooling and change the optimizer to adam and adjust learning rate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvolutionalNet(nn.Module):\n",
    "    \"\"\"Convolutional neural network\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepConvolutionalNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net2 = DeepConvolutionalNet()\n",
    "print(net2)\n",
    "hl_graph = hl.build_graph(net2, torch.zeros([1, 1, 28, 28]), transforms=transforms)\n",
    "hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()\n",
    "hl_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = hl.History()\n",
    "canvas2 = hl.Canvas()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.001)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(net2, train_loader, optimizer, epoch, history2, canvas2, history1)\n",
    "    test(net2, test_loader)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary we loaded the MNIST dataset, built a neural network using pyTorch, made training and testing loops for our data and trained the network using pyTorch backpropagation and stochastic gradient descent. Finally we increased the capacity of the network by adding layers and changed our optimizer to Adam. Now it's your turn! Can you find even better hyperparameters? Try changing the batch_size and learning_rate and if you want you can try adding convolutional and dropout layers to the network.\n",
    "\n",
    "Congratulations, you reached the end of this tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
