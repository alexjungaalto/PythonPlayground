{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Bitville 2019\n",
    "\n",
    "After this tutorial, you will understand the philosophy behind the deep learning paradigm in artificial intelligence and be able to apply deep learning to solve real world problems. Because of the availability of good deep learning libraries, you should be able to complete this tutorial if you know the basics of computer programming.\n",
    "\n",
    "To implement a deep learning pipeline, we need to solve the following subproblems:\n",
    "\n",
    "1. Collecting <b>data</b> and choosing how to represent it.\n",
    "2. Choosing how to <b>model</b> the data.\n",
    "3. Choosing how to measure how good the model is with a <b>cost function</b>.\n",
    "4. Choosing how to <b>optimize</b> the cost function given the model, data and cost function.\n",
    "5. Choosing how to <b>validate</b> how good the final learned model is.\n",
    "\n",
    "We will go through all of these steps, and show how they can be implemented in python by the aid of a few software libraries. After this you will get an instructed exercise to improve the pipeline to get a better performing model.\n",
    "\n",
    "You can run the individual cells below by clicking the cell and the pressing \"shift\" + \"enter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the software tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use <b>pytorch</b> as our deep learning library. In addition, we will use <b>numpy</b> as our math library and <b>matplotlib</b> as our plotting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # The pytorch deep learning library\n",
    "import torchvision # Computer vision module\n",
    "import torch.nn as nn # Neural network tools\n",
    "import torch.nn.functional as F # More math functions\n",
    "import torch.optim as optim # Optimizer\n",
    "from torch.autograd import Variable # Variables for automatic differentiation\n",
    "from torchvision import datasets, transforms # Helper modules for loading datasets\n",
    "import numpy as np # Math library\n",
    "import matplotlib.pyplot as plt # Plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the MNIST data set, the most studied data set in deep learning. The MNIST data set consists of handwritten digits, and the problem is to create a model that is able to automatically <b>classify</b> new digits into 10 discrete classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loading of data only needs to be understood at a conceptual level\n",
    "\n",
    "# trainset is an object that represents the training set\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                      download=True, transform=transforms.ToTensor())\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# train_iterator is an object that automatically loads training data for us\n",
    "train_iterator = iter(torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True))\n",
    "\n",
    "\n",
    "# test_iterator is an object that automatically loads training data for us\n",
    "test_iterator = iter(torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now get a batch of data by calling the next() method of the iterator\n",
    "images, labels = train_iterator.next()\n",
    "\n",
    "print(\"images tensor shape: {}\".format(images.size()))\n",
    "print(\"labels tensor shape: {}\\n\".format(labels.size()))\n",
    "print(\"A data point with the label {} looks like this:\".format(labels[0]))\n",
    "plt.imshow(images[0,0,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <i>images</i> and <i>labels</i> variables are pytorch tensors. Tensor objects can be thought of as datastructures that represent multidimensional arrays. Following this logic, a scalar is a rank 0 tensor, a vector is a rank 1 tensor, a matrix is rank 2 and so on. The <i>images</i> tensor has rank 4, the two inner dimensions (axis 2 and 3) denote the horizontal and vertical pixels, axis 1 denotes the color channels and axis 0 denotes the data samples. A collection of data samples is called a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to represent our data is usually determined by our model. In deep learning, we usually want to preprocess the data as little as possible and offload this job to the model itself. Representing the data in an appropriate way can however make the job for the model much easier. A very common preprocessing step is to normalize the input values, but this time the values are already in a suitable range. In this case, our only needed task is to <b>flatten</b> the data to a rank 2 tensor. We will do this step when we feed the data in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of a model is to encode our assumptions about the structural form of our data. To make learning tractable, we need to constrain the the set of interpretations about the data that our model can make. More formally, under-constraining our model leads to high amounts of variance in the model’s interpretation of the data. Assumptions in our model introduce invariance (i.e. bias), and can be very effective at lowering variance. In the deep learning framework, assumptions about structural forms of data are referred to as inductive biases, because they induce bias to fight variance.\n",
    "\n",
    "We are going to use a simple kind of a neural network as our model, namely a multilayer perceptron (MLP). An MLP makes quite weak assumptions about the data, and as such does not scale very well to more difficult problems. One can think of an MLP as a sequence of simple mathematical functions that map an input to a class label. The MLP is parametrised by variables (called weights) that determine how an input will be mapped to an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class that defines the neural network must be in certain format in pytorch\n",
    "# The class Mlp inherits from nn.Module that has important properties\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self):                 \n",
    "        super(Mlp, self).__init__()\n",
    "        print('constructing model for a MLP network...')\n",
    "        \n",
    "        # The following lines define the fully connected network layers\n",
    "        self.fc1 = nn.Linear(784,15)\n",
    "        self.fc2 = nn.Linear(15,10)\n",
    "    \n",
    "    \n",
    "    # The following part describes the forward pass and thus the structure of the neural net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)           #Reshape input tensor.\n",
    "        x = F.relu(self.fc1(x))         #Fully connected hidden layer.\n",
    "        x = self.fc2(x)                 #10 fully connected output neurons for our digits. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()                   # define what cost function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000  # how many iterations in our training loop\n",
    "\n",
    "train_iterator = iter(torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True))  # reset the train data iterator\n",
    "test_iterator = iter(torch.utils.data.DataLoader(testset, batch_size=500, shuffle=True))   # reset the test data iterator\n",
    "net = Mlp()  # create instance of the Mlp class\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01) # define what optimizer to use\n",
    "\n",
    "# let us gather the loss and accuracy values starting from empty lists\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "# the training loop starts here\n",
    "for i in range(iterations):\n",
    "    images, labels = train_iterator.next()  # get a batch of data from the iterator\n",
    "    optimizer.zero_grad()  # in pytorch, we need to zero the gradients in each iteration\n",
    "    output = net.forward(images)  # output from our simple neural net\n",
    "    loss = criterion(output, labels) # calculating the loss\n",
    "    loss.backward() # calculating the backpropagation\n",
    "    optimizer.step() # updating the parameter values\n",
    "\n",
    "    losses.append(loss) # collect loss values for plotting\n",
    "\n",
    "    if i % 100 == 0: # print loss values every 100th iteration on the screen\n",
    "        \n",
    "        \n",
    "        images_test, labels_test = test_iterator.next()\n",
    "        output_test = net.forward(images_test)\n",
    "        y_pred = torch.argmax(output_test, dim=1)\n",
    "        accuracy = torch.sum(y_pred==labels_test).detach().numpy()/500\n",
    "        accuracies.append(accuracy)\n",
    "        print('iteration {}, loss: {}, accuracy: {}'.format(i,loss, accuracy))\n",
    "\n",
    "\n",
    "# let us print the final accuracy after the trainign loop is finished\n",
    "images_test, labels_test = test_iterator.next()\n",
    "output_test = net.forward(images_test)\n",
    "y_pred = torch.argmax(output_test, dim=1)\n",
    "accuracy = torch.sum(y_pred==labels_test).detach().numpy()/500\n",
    "print('Test accuracy now is {}'.format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For proper evaluation, let us feed in the entire test set (10000 samples) and measure the accuracy\n",
    "test_iterator = iter(torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=True))   # reset the test data iterator\n",
    "\n",
    "images_test, labels_test = test_iterator.next()\n",
    "output_test = net.forward(images_test)\n",
    "y_pred = torch.argmax(output_test, dim=1)\n",
    "accuracy = torch.sum(y_pred==labels_test).detach().numpy()/10000\n",
    "print('FINAL TEST ACCURACY IS: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss values during the iterations\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the test set accuracy during the iterations\n",
    "plt.plot(accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably reached a bit better accuracy than 80% with these hyperparameters. The important hyperparameters in our case are the dimensions of the neural network, i.e how many layers and how many neurons in each layer, and the learning rate. \n",
    "\n",
    "Try to change the network dimensions by adding another layer and experimenting with different amount of neurons in each layer.\n",
    "\n",
    "Experiment also with the learning rate value. Observe how the learning rate affects the learning curve, i.e how fast the loss decreases and accuracy increases.\n",
    "\n",
    "There are also more advanced optimizers in PyTorch that you can experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
